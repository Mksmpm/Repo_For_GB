{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857bc49c",
   "metadata": {},
   "source": [
    "### Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы получаем должность) с сайтов HH(обязательно) и/или Superjob(по желанию). Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:\n",
    "- Наименование вакансии.\n",
    "- Предлагаемую зарплату (разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам).\n",
    "- Ссылку на саму вакансию.\n",
    "- Сайт, откуда собрана вакансия. (можно прописать статично hh.ru или superjob.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb851153",
   "metadata": {},
   "source": [
    "#### Import all the necessary libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7c4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360e451",
   "metadata": {},
   "source": [
    "#### Job title constant for hh.ru scraping (or can be arg in .py script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f889f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = \"Data engineer\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa90f4",
   "metadata": {},
   "source": [
    "#### Function that returns an appropriate url link for scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd236c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_url(job_title, page=0):\n",
    "    #job_title = \"Data engineer\"\n",
    "    url = \"https://hh.ru/search/vacancy\"\n",
    "    params = {\n",
    "    \"text\": job_title,\n",
    "    \"from\":\"suggest_post\",\n",
    "    \"clusters\":\"true\",\n",
    "    \"ored_clusters\":\"true\",\n",
    "    \"enable_snippets\":\"true\",\n",
    "    \"page\":page}\n",
    "    params_encoded = urlencode(params)\n",
    "    return f\"{url}?{params_encoded}\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4025282",
   "metadata": {},
   "source": [
    "#### Function that returns a DOM object by going to a specific page using generic url link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31021a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dom_from_url(job_title, session, page = 0):\n",
    "    headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = session.get(prepare_url(job_title, page), headers=headers)\n",
    "    return BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa46609",
   "metadata": {},
   "source": [
    "#### Function that returns an integer value of maximum pages for current \"job_title\" url search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5252dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_max_pages(job_title, session):\n",
    "    response = get_dom_from_url(job_title, session)\n",
    "    return int(response.find(\"a\",{\"class\":\"bloko-button\",\n",
    "                              \"data-qa\":\"pager-next\" }).previous_sibling.find('a',{\"data-qa\":\"pager-page\"}).text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7315b6",
   "metadata": {},
   "source": [
    "#### Function that scraps through the vacancies on the page and writes it into the vacancy_list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f842124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_page(vacancies, vacancy_list):\n",
    "    for vacancy in vacancies:\n",
    "            vacancy_info = {}\n",
    "            vacancy_data = vacancy.find(\"div\", {\"class\":\"\"})\n",
    "            vacancy_titile = vacancy_data.find(\"a\", {\"data-qa\":\"vacancy-serp__vacancy-title\"}).text\n",
    "            vacancy_link = vacancy_data.find(\"a\", {\"data-qa\":\"vacancy-serp__vacancy-title\"}).get(\"href\")\n",
    "            if vacancy_data.find(\"span\", {\"data-qa\":\"vacancy-serp__vacancy-compensation\"}) == None: # if salary is None then hardcode None\n",
    "                vacancy_salary = None\n",
    "                salary_from = None\n",
    "                salary_to = None\n",
    "                salary_currency = None\n",
    "            else:\n",
    "                vacancy_salary = vacancy_data.find(\"span\", {\"data-qa\":\"vacancy-serp__vacancy-compensation\"}).text\n",
    "                salary_currency = vacancy_salary.split(\" \")[-1].replace(\".\",\"\")\n",
    "                if (\"от\" in vacancy_salary):\n",
    "                    salary_from = int(vacancy_salary.split(\" \")[1].replace(\"\\u202f\", \"\"))\n",
    "                    salary_to = None\n",
    "                elif \"до\" in vacancy_salary:\n",
    "                    salary_from = None\n",
    "                    salary_to = int(vacancy_salary.split(\" \")[1].replace(\"\\u202f\", \"\"))\n",
    "                elif \"–\" in vacancy_salary:\n",
    "                    salary_from = int(vacancy_salary.split(\" \")[0].replace(\"\\u202f\", \"\"))\n",
    "                    salary_to = int(vacancy_salary.split(\" \")[2].replace(\"\\u202f\", \"\"))\n",
    "                else:\n",
    "                    salary_from = None\n",
    "                    salary_to = None\n",
    "            vacancy_info[\"vacancy_titile\"] = vacancy_titile\n",
    "            vacancy_info[\"salary_from\"] = salary_from\n",
    "            vacancy_info[\"salary_to\"] = salary_to\n",
    "            vacancy_info[\"salary_currency\"] = salary_currency\n",
    "            vacancy_info[\"vacancy_link\"] = vacancy_link\n",
    "            vacancy_info[\"site_from\"] = \"hh.ru\"\n",
    "            vacancy_list.append(vacancy_info)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4450305",
   "metadata": {},
   "source": [
    "#### Function that iterates through the available pages and runs the \"scrap_page()\" function for each applicable url page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdfca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_pages(job_title, session, vacancy_list):\n",
    "    max_pages = get_search_max_pages(job_title, session)\n",
    "    print(f\"Starting scraping through pages for {job_title} position\")\n",
    "    for page in range(max_pages):\n",
    "        print(f\"Scraping page {page+1} out of {max_pages}...\")\n",
    "        page_dom = get_dom_from_url(job_title, session, page)\n",
    "        vacancies = page_dom.find_all(\"div\", {\"class\":\"vacancy-serp-item-body__main-info\"})\n",
    "        scrap_page(vacancies, vacancy_list)\n",
    "    print(f\"Scraping is finished, found total of {len(vacancy_list)} vacancies\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cdafe",
   "metadata": {},
   "source": [
    "#### Function that writes all the scraped data throught pages into one .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a5083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(job_title, vacancy_list):\n",
    "    df = pd.DataFrame(vacancy_list)\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"{current_time}_{job_title}_HH_Vacancies.csv\"\n",
    "    df.to_csv(filename, sep=';', float_format='%.2f', index=False)\n",
    "    print(f'File \"{filename}\" successfully generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69e6e7",
   "metadata": {},
   "source": [
    "#### Main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5531155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(job_title):\n",
    "    vacancy_list = []\n",
    "    session = requests.Session()\n",
    "    scrap_pages(job_title, session, vacancy_list)\n",
    "    write_to_csv(job_title, vacancy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f89561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping through pages for Data engineer position\n",
      "Scraping page 1 out of 31...\n",
      "Scraping page 2 out of 31...\n",
      "Scraping page 3 out of 31...\n",
      "Scraping page 4 out of 31...\n",
      "Scraping page 5 out of 31...\n",
      "Scraping page 6 out of 31...\n",
      "Scraping page 7 out of 31...\n",
      "Scraping page 8 out of 31...\n",
      "Scraping page 9 out of 31...\n",
      "Scraping page 10 out of 31...\n",
      "Scraping page 11 out of 31...\n",
      "Scraping page 12 out of 31...\n",
      "Scraping page 13 out of 31...\n",
      "Scraping page 14 out of 31...\n",
      "Scraping page 15 out of 31...\n",
      "Scraping page 16 out of 31...\n",
      "Scraping page 17 out of 31...\n",
      "Scraping page 18 out of 31...\n",
      "Scraping page 19 out of 31...\n",
      "Scraping page 20 out of 31...\n",
      "Scraping page 21 out of 31...\n",
      "Scraping page 22 out of 31...\n",
      "Scraping page 23 out of 31...\n",
      "Scraping page 24 out of 31...\n",
      "Scraping page 25 out of 31...\n",
      "Scraping page 26 out of 31...\n",
      "Scraping page 27 out of 31...\n",
      "Scraping page 28 out of 31...\n",
      "Scraping page 29 out of 31...\n",
      "Scraping page 30 out of 31...\n",
      "Scraping page 31 out of 31...\n",
      "Scraping is finished, found total of 620 vacancies\n",
      "File \"2022-07-17_13-54-49_Data engineer_HH_Vacancies.csv\" successfully generated\n"
     ]
    }
   ],
   "source": [
    "main(job_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
